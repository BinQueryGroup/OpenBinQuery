{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from common import mean_pooling\n",
    "from transformers import MPNetModel\n",
    "from datasets import load_from_disk\n",
    "from model import AsmEncoder\n",
    "from pprint import pprint\n",
    "from openai import OpenAI\n",
    "import pickle\n",
    "import torch\n",
    "import json\n",
    "import zstd\n",
    "import sys\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We delete the output of the block above, as it will output a warning prompt containing identity information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompress_data(b_str):\n",
    "    return pickle.loads(zstd.decompress(b_str))\n",
    "def compress_data(obj):\n",
    "    return zstd.compress(pickle.dumps(obj))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We used our model weights when executing this notebook, but did not make them public."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (os.path.exists(\"models/binquery/asm\") and os.path.join(\"models/binquery/desc\")):\n",
    "    print(\"This Jupyter notebook is just a demonstration, the model weights will be released when the paper is officially published.\")\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'CodeRangeTokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'MPNetTokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "asm_model = AsmEncoder.from_pretrained(\"models/binquery/asm\")\n",
    "desc_model = MPNetModel.from_pretrained(\"models/binquery/desc\")\n",
    "asm_tokenizer = PreTrainedTokenizerFast.from_pretrained(\"tokenizers/asm\")\n",
    "desc_tokenizer = PreTrainedTokenizerFast.from_pretrained(\"tokenizers/desc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset/vulsearch/task.json\") as f:\n",
    "    task = json.load(f)\n",
    "target = task[\"target\"]\n",
    "query = task[\"cve_description\"]\n",
    "project = task['project']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_from_disk(f\"dataset/vulsearch/{project}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute embedding vectors for all binary functions in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [00:43<00:00,  1.11s/it]\n"
     ]
    }
   ],
   "source": [
    "candidate_list = [\"\\n\".join(decompress_data(row[\"asm\"])) for row in ds]\n",
    "tokenized_candidate = asm_tokenizer(candidate_list, truncation=True, return_tensors=\"pt\", padding=\"max_length\", max_length=1024)\n",
    "asm_model = asm_model.cuda()\n",
    "asm_model.eval()\n",
    "asm_model.requires_grad_(False)\n",
    "embedding_list = []\n",
    "with torch.no_grad():\n",
    "    for start in tqdm(range(0, len(candidate_list), 64)):\n",
    "        input_ids = tokenized_candidate[\"input_ids\"][start:start+64]\n",
    "        attention_mask = tokenized_candidate[\"attention_mask\"][start:start+64]\n",
    "        token_type_ids = tokenized_candidate[\"token_type_ids\"][start:start+64]\n",
    "        input_ids = input_ids.cuda()\n",
    "        attention_mask = attention_mask.cuda()\n",
    "        token_type_ids = token_type_ids.cuda()\n",
    "        # output = asm_model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        output = asm_model(input_ids, attention_mask=attention_mask)\n",
    "        embedding_list.append(mean_pooling(output.last_hidden_state.cpu(), attention_mask))\n",
    "candidate_embeddings = torch.cat(embedding_list, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the environment for query augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset/secret.json\") as f:\n",
    "    secret = json.load(f)\n",
    "LLM_KEY = secret[\"LLM_KEY\"]\n",
    "LLM_URL = secret[\"LLM_URL\"]\n",
    "LLM_MODEL = secret[\"LLM_MODEL\"]\n",
    "TEMPERATURE = 1\n",
    "MAX_TOKENS = 8192\n",
    "TIMEOUT = 60\n",
    "with open(\"dataset/prompts/augmentation.txt\") as f:\n",
    "    SYSTEM_PROMPT = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=LLM_KEY, base_url=LLM_URL)\n",
    "def complete(user: str):\n",
    "    cnt = 0\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": SYSTEM_PROMPT,\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": user,\n",
    "                },\n",
    "            ],\n",
    "            timeout=TIMEOUT,\n",
    "            model=LLM_MODEL,\n",
    "            temperature=TEMPERATURE,\n",
    "            max_tokens=MAX_TOKENS,\n",
    "        )\n",
    "        result = completion.choices[0].message.content\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json(s):\n",
    "    pattern = r\"```[\\w\\s]*\\n(.*?)```\"\n",
    "    match = re.search(pattern, s, re.DOTALL)\n",
    "    if match:\n",
    "        code_block = match.group(1).strip()\n",
    "        return eval(code_block)\n",
    "    else:\n",
    "        raise Exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_query(query: str):\n",
    "    user_input = \"Original query: \" + query\n",
    "    response = complete(user_input)\n",
    "    result = get_json(response)['description']\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Natural Language-based Binary Function Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Target: 3'\n",
      "('Original Query: Multiple use-after-free vulnerabilities in the (1) '\n",
      " 'htmlPArsePubidLiteral and (2) htmlParseSystemiteral functions in libxml2 '\n",
      " 'before 2.9.4, as used in Apple iOS before 9.3.2, OS X before 10.11.5, tvOS '\n",
      " 'before 9.2.1, and watchOS before 2.2.1, allow remote attackers to cause a '\n",
      " 'denial of service via a crafted XML document.')\n"
     ]
    }
   ],
   "source": [
    "pprint(f\"Target: {target}\")\n",
    "pprint(f\"Original Query: {query}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Augmented Query: Implements two functions, (1) htmlParsePubidLiteral and (2) '\n",
      " 'htmlParseSystemLiteral, which parse public and system identifiers in XML '\n",
      " 'documents, respectively. These functions are designed to handle specific XML '\n",
      " 'syntax elements and ensure proper parsing of identifiers within XML '\n",
      " 'documents. The functions are part of the libxml2 library, which is used in '\n",
      " 'various Apple operating systems for XML processing.')\n"
     ]
    }
   ],
   "source": [
    "augmented_query = augment_query(query)\n",
    "pprint(f\"Augmented Query: {augmented_query}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of original query is ranked 1131 out of 2477\n"
     ]
    }
   ],
   "source": [
    "tokenized = desc_tokenizer(query, truncation=True, return_tensors=\"pt\", padding=\"max_length\", max_length=1024)\n",
    "query_embedding = mean_pooling(desc_model(input_ids=tokenized[\"input_ids\"], attention_mask=tokenized[\"attention_mask\"]).last_hidden_state, tokenized[\"attention_mask\"])\n",
    "scores = torch.matmul(candidate_embeddings, query_embedding.T)\n",
    "target_score = scores[task['target']]\n",
    "rank = (scores > target_score).sum().item() + 1\n",
    "print(f\"The result of original query is ranked {rank} out of {scores.size(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of augmented query is ranked 30 out of 2477\n"
     ]
    }
   ],
   "source": [
    "tokenized = desc_tokenizer(augmented_query, truncation=True, return_tensors=\"pt\", padding=\"max_length\", max_length=1024)\n",
    "query_embedding = mean_pooling(desc_model(input_ids=tokenized[\"input_ids\"], attention_mask=tokenized[\"attention_mask\"]).last_hidden_state, tokenized[\"attention_mask\"])\n",
    "scores = torch.matmul(candidate_embeddings, query_embedding.T)\n",
    "target_score = scores[task['target']]\n",
    "rank = (scores > target_score).sum().item() + 1\n",
    "print(f\"The result of augmented query is ranked {rank} out of {scores.size(0)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
