{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tree_sitter import Language, Parser, Node\n",
    "from datasets import load_from_disk\n",
    "import tree_sitter_cpp\n",
    "import pickle, zstd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We delete the output of the block above, as it will output a warning prompt containing identity information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompress_data(b_str):\n",
    "    return pickle.loads(zstd.decompress(b_str))\n",
    "def compress_data(obj):\n",
    "    return zstd.compress(pickle.dumps(obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpp_language = Language(tree_sitter_cpp.language())\n",
    "def traverse_node(node: Node, mode: str = \"post_order\"):\n",
    "    if mode not in [\"post_order\", \"depth_first\"]:\n",
    "        raise ValueError(\"mode must be either post_order or depth_first\")\n",
    "    cursor = node.walk()\n",
    "    while True:\n",
    "        if mode == \"depth_first\":\n",
    "            yield cursor.node\n",
    "        if cursor.goto_first_child():\n",
    "            continue\n",
    "        if mode == \"post_order\":\n",
    "            yield cursor.node\n",
    "        if cursor.goto_next_sibling():\n",
    "            continue\n",
    "        while True:\n",
    "            if not cursor.goto_parent():\n",
    "                return\n",
    "            if mode == \"post_order\":\n",
    "                yield cursor.node\n",
    "            if cursor.goto_next_sibling():\n",
    "                break\n",
    "def extract_snippets(code):\n",
    "    parser = Parser(language=cpp_language)\n",
    "    tree = parser.parse(code.encode())\n",
    "    skip = []\n",
    "    results = []\n",
    "    # [\"for_statement\", \"while_statement\", \"do_statement\", \"if_statement\", \"compound_statement\"]:\n",
    "    for node in traverse_node(tree.root_node):\n",
    "        if node in skip:\n",
    "            continue\n",
    "        if node.type == \"function_definition\":\n",
    "            body = node.child_by_field_name(\"body\")\n",
    "            if body and body.type == \"compound_statement\":\n",
    "                skip.append(body)\n",
    "        if node.type in [\n",
    "            \"for_statement\",\n",
    "            \"while_statement\",\n",
    "            \"do_statement\",\n",
    "            \"if_statement\",\n",
    "            \"compound_statement\",\n",
    "        ]:\n",
    "            results.append((node.start_point.row, node.end_point.row + 1))\n",
    "    return list(set(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_extract_snippets(row):\n",
    "    src_list = decompress_data(row[\"src\"])\n",
    "    src_s = \"\\n\".join(src_list)\n",
    "    snippets = extract_snippets(src_s)\n",
    "    return {\"snippets_from_rule\": snippets}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function do_extract_snippets at 0x7cc1bd98f560> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "Map (num_proc=20): 100%|██████████| 100/100 [00:00<00:00, 391.11 examples/s]\n"
     ]
    }
   ],
   "source": [
    "ds = load_from_disk(\"data/llm_extract_snippets\")\n",
    "ds = ds.map(do_extract_snippets, num_proc=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 100/100 [00:00<00:00, 2703.82 examples/s]\n"
     ]
    }
   ],
   "source": [
    "ds.save_to_disk(\"data/rule_llm_extract_snippets\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
